{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Task 1. Image classification + OOP</b></center>\n",
    "\n",
    "In this task, you need to use a publicly available simple MNIST dataset and build 3 classification\n",
    "models around it. It should be the following models:\n",
    "\n",
    "1) Random Forest;\n",
    "2) Feed-Forward Neural Network;\n",
    "3) Convolutional Neural Network;\n",
    "\n",
    "Each model should be a separate class that implements MnistClassifierInterface with 2 abstract methods - train and predict. Finally, each of your three models should be hidden under\n",
    "\n",
    "another MnistClassifier class. MnistClassifer takes an algorithm as an input parameter. Possible values for the algorithm are: cnn, rf, and nn for the three models described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement requirements.txt (from versions: none)\n",
      "ERROR: No matching distribution found for requirements.txt\n",
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MnistClassifier Interface that used for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class MnistClassifierInterface(ABC):\n",
    "    \"\"\"\n",
    "    MnistClassifierInterface includes 2 methods for train model and prediction.\n",
    "    It inherited by CNNClassifier, RFClassifier, FFNNClassifier\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def train(self,x_train,y_train):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self,x_test):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> First model : Random forest</b>\n",
    "\n",
    "After creation of interface, now we can create each of models, which will be separated in classes. The first model will be Random Forest.\n",
    "\n",
    "Model was created with sklearn and trained using KFold algorithm separating dataset into 10 folds. During the fold, for searching the best parameters of model, i used the RandomizedSearchCV,\n",
    "\n",
    "where I put parameters and possible values for them. This function will find the best combination of parameters' values. Then, the model will be compared to the accuracy score of the best model that was find at that moment.\n",
    "\n",
    "Dataset for training and prediction was initialized in file for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MnistClassifierInterface import MnistClassifierInterface\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "class RFClassifier(MnistClassifierInterface):\n",
    "\n",
    "    \"\"\"\n",
    "    Random Forest model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model=RandomForestClassifier()\n",
    "\n",
    "    def train(self,x_train,y_train):\n",
    "\n",
    "        self.model.fit(x_train, y_train)\n",
    "        best_accuracy_score=0\n",
    "\n",
    "        accuracy_train_arr, accuracy_test_arr = [], []\n",
    "        kf = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "\n",
    "            X_train_fold, X_test_fold = x_train[train_index],x_train[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "\n",
    "            distributions = {\n",
    "                'n_estimators': [10, 50, 100, 200, 300],\n",
    "                'max_depth': [9, 15, 20, 30],\n",
    "                'criterion' : ['gini','entropy'],\n",
    "                'min_samples_leaf': [1,5,10,20,50,100],\n",
    "                'warm_start': [True,False],\n",
    "                'bootstrap': [True, False],\n",
    "                'min_samples_leaf': [1,5,10,20,50,100],\n",
    "                'max_leaf_nodes': [5,10,20,50,100,200],\n",
    "\n",
    "            }\n",
    "\n",
    "            clf = RandomizedSearchCV(self.model, distributions,\n",
    "                                     verbose=0, scoring='accuracy')\n",
    "            search = clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            model = search.best_estimator_\n",
    "\n",
    "            print('Search is finished. Best parameters: \\n')\n",
    "            print(search.best_params_)\n",
    "\n",
    "\n",
    "            y_pred = model.predict(X_train_fold)\n",
    "            accuracy_train = accuracy_score(y_train_fold, y_pred)\n",
    "\n",
    "            y_pred = model.predict(X_test_fold)\n",
    "            accuracy_test = accuracy_score(y_test_fold, y_pred)\n",
    "\n",
    "            accuracy_train_arr.append(accuracy_train)\n",
    "            accuracy_test_arr.append(accuracy_test)\n",
    "\n",
    "            print(f'\\n FOLD-{i}. Accuracy train: {accuracy_train}, Accuracy test: {accuracy_test} \\n')\n",
    "\n",
    "\n",
    "            if accuracy_test > best_accuracy_score:\n",
    "                best_accuracy_score = accuracy_test\n",
    "                best_model = model\n",
    "                print(f' FOLD-{i}. Model has best score.')\n",
    "\n",
    "        self.model = best_model\n",
    "\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return self.model.predict(x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Second model : Feedforward neural network</b>\n",
    "\n",
    "Model was created with keras and tensorflow-directml (to be able to use GPU during model training).  \n",
    "Architecture of the model consist of: \n",
    "* Input of 64 array,\n",
    "\n",
    "* Dense layers with 128, 256, 128, 64 neurons, respectively, with relu as activation function,\n",
    "\n",
    "* The final layer is a dense of 10 neurons with softmax as activation function. \n",
    "As a loss function for training of this model was used the \"sparse_categorical_crossentropy\" and Adam as an optimizer. \n",
    "\n",
    "Dataset for training and prediction was initialized in file for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.keras\n",
    "from MnistClassifierInterface import MnistClassifierInterface\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input\n",
    "import numpy as np\n",
    "class FFNNClassifier(MnistClassifierInterface):\n",
    "\n",
    "    \"\"\"\n",
    "    Feedforward Neural Network model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        input=Input(shape=(64,),dtype=tf.float64)\n",
    "\n",
    "        dense=Dense(128, activation='relu',name='dense_layer_1')(input)\n",
    "        dense = Dense(256, activation='relu', name='dense_layer_2')(dense)\n",
    "        dense = Dense(128, activation='relu', name='dense_layer_3')(dense)\n",
    "        dense = Dense(64, activation='relu', name='dense_layer_4')(dense)\n",
    "        output=Dense(10,activation='softmax',name='output_layer')(dense)\n",
    "\n",
    "        self.model=Model(inputs=input, outputs=output)\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self,x_train,y_train):\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(x_train, y_train,epochs=50, batch_size=32)\n",
    "    def predict(self,x_test):\n",
    "        self.model.predict(x_test)\n",
    "        y_pred = self.model.predict(x_test)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Third model : Convolution neural network</b>\n",
    "\n",
    "Model was created with keras and tensorflow-directml (to be able to use GPU during model training).  \n",
    "\n",
    "Architecture of the model consist of: \n",
    "* input of (8,8,1) tensor, \n",
    "\n",
    "* Convolution layer with 16 filters of (3,3) size with relu activation, \n",
    "\n",
    "* MaxPooling layer, \n",
    "\n",
    "* Convoluition layer with 32 filters with relu activation, \n",
    "\n",
    "* Flatten layer and then Dense layers with 256, 128, 64 neurons, respectively, with relu as activation function, \n",
    "\n",
    "* The final layer is a dense of 10 neurons with softmax as activation function. \n",
    "\n",
    "As a loss function for training of this model was used the \"sparse_categorical_crossentropy\" and Adam as an optimizer. \n",
    "\n",
    "Dataset for training and prediction was initialized in file for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MnistClassifierInterface import MnistClassifierInterface\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input, Conv2D,MaxPooling2D\n",
    "import numpy as np\n",
    "class CNNClassifier(MnistClassifierInterface):\n",
    "    \"\"\"\n",
    "    Convolution Neural Network model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        input = Input(shape=(8,8,1), dtype=tf.float64)\n",
    "\n",
    "        conv1 = Conv2D(16,(3,3), activation='relu', name='conv_layer_1')(input)\n",
    "        pool1 = MaxPooling2D((1,1),strides=None, padding=\"valid\", name='maxpool_layer_1')(conv1)\n",
    "        conv2 = Conv2D(32, (3, 3), activation='relu', name='conv_layer_2')(pool1)\n",
    "        flatten=Flatten()(conv2)\n",
    "        dense = Dense(256, activation='relu', name='dense_layer_2')(flatten)\n",
    "        dense = Dense(128, activation='relu', name='dense_layer_5')(dense)\n",
    "        dense = Dense(64, activation='relu', name='dense_layer_6')(dense)\n",
    "        output = Dense(10, activation='softmax', name='output_layer')(dense)\n",
    "\n",
    "        self.model = Model(inputs=input, outputs=output)\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self,x_train,y_train):\n",
    "        x_train=x_train.reshape((x_train.shape[0],8, 8))\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(x_train, y_train,epochs=30, batch_size=32)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        x_test = x_test.reshape((x_test.shape[0], 8, 8))\n",
    "        self.model.predict(x_test)\n",
    "        y_pred = self.model.predict(x_test)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model inference </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model is called\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': True, 'n_estimators': 300, 'min_samples_leaf': 1, 'max_leaf_nodes': 50, 'max_depth': 30, 'criterion': 'gini', 'bootstrap': True}\n",
      "\n",
      " FOLD-0. Accuracy train: 0.9893899204244032, Accuracy test: 0.9682539682539683 \n",
      "\n",
      " FOLD-0. Model has best score.\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 100, 'min_samples_leaf': 1, 'max_leaf_nodes': 50, 'max_depth': 30, 'criterion': 'gini', 'bootstrap': True}\n",
      "\n",
      " FOLD-1. Accuracy train: 0.9885057471264368, Accuracy test: 0.9126984126984127 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 50, 'min_samples_leaf': 10, 'max_leaf_nodes': 200, 'max_depth': 9, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      " FOLD-2. Accuracy train: 0.9849690539345711, Accuracy test: 0.9841269841269841 \n",
      "\n",
      " FOLD-2. Model has best score.\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 200, 'min_samples_leaf': 5, 'max_leaf_nodes': 200, 'max_depth': 30, 'criterion': 'entropy', 'bootstrap': True}\n",
      "\n",
      " FOLD-3. Accuracy train: 0.9973474801061007, Accuracy test: 0.9682539682539683 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': True, 'n_estimators': 100, 'min_samples_leaf': 1, 'max_leaf_nodes': 100, 'max_depth': 15, 'criterion': 'entropy', 'bootstrap': True}\n",
      "\n",
      " FOLD-4. Accuracy train: 1.0, Accuracy test: 0.9920634920634921 \n",
      "\n",
      " FOLD-4. Model has best score.\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 200, 'min_samples_leaf': 1, 'max_leaf_nodes': 50, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': True}\n",
      "\n",
      " FOLD-5. Accuracy train: 0.9929266136162688, Accuracy test: 0.9365079365079365 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': True, 'n_estimators': 50, 'min_samples_leaf': 5, 'max_leaf_nodes': 50, 'max_depth': 9, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      " FOLD-6. Accuracy train: 0.9920424403183024, Accuracy test: 0.9841269841269841 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 200, 'min_samples_leaf': 5, 'max_leaf_nodes': 100, 'max_depth': 15, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      " FOLD-7. Accuracy train: 0.9991166077738516, Accuracy test: 0.984 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': True, 'n_estimators': 50, 'min_samples_leaf': 5, 'max_leaf_nodes': 200, 'max_depth': 20, 'criterion': 'gini', 'bootstrap': True}\n",
      "\n",
      " FOLD-8. Accuracy train: 0.9946996466431095, Accuracy test: 0.928 \n",
      "\n",
      "Search is finished. Best parameters: \n",
      "\n",
      "{'warm_start': False, 'n_estimators': 200, 'min_samples_leaf': 5, 'max_leaf_nodes': 50, 'max_depth': 9, 'criterion': 'gini', 'bootstrap': False}\n",
      "\n",
      " FOLD-9. Accuracy train: 0.9858657243816255, Accuracy test: 0.976 \n",
      "\n",
      "Accuracy score for test data: 0.9648148148148148\n"
     ]
    }
   ],
   "source": [
    "from MnistClassifier import MnistClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data = load_digits() # Load MNIST dataset\n",
    "X = data.data/255\n",
    "Y = data.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y, test_size=0.3) #Split dataset into train and test sets\n",
    "\n",
    "model=MnistClassifier(algorithm='rf') #initialize one of three models: rf,cnn or nn.\n",
    "model.train(x_train,y_train) #fit the train data\n",
    "y_pred=model.predict(x_test) #predict test data\n",
    "acc_score = accuracy_score(y_test, y_pred) #evaaluate metric\n",
    "print(f'Accuracy score for test data: {acc_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
