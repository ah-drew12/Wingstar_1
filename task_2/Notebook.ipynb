{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Task 2. Named entity recognition + image classification</b></center>\n",
    "\n",
    "In this task, you will work on building your ML pipeline that consists of 2 models responsible for\n",
    "totally different tasks. The main goal is to understand what the user is asking (NLP) and check if\n",
    "he is correct or not (Computer Vision).\n",
    "\n",
    "You will need to:\n",
    "\n",
    "* find or collect an animal classification/detection dataset that contains at least 10\n",
    "classes of animals.\n",
    "* train NER model for extracting animal titles from the text. Please use some\n",
    "transformer-based model (not LLM).\n",
    "* Train the animal classification model on your dataset.\n",
    "* Build a pipeline that takes as inputs the text message and the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: aiohttp==3.11.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 3)) (3.11.13)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: async-timeout==5.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: attrs==25.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 7)) (25.1.0)\n",
      "Requirement already satisfied: cachetools==5.5.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 8)) (5.5.2)\n",
      "Requirement already satisfied: certifi==2025.1.31 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: contourpy==1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: datasets==3.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 14)) (3.3.2)\n",
      "Requirement already satisfied: dill==0.3.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 15)) (0.3.8)\n",
      "Requirement already satisfied: filelock==3.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 16)) (3.17.0)\n",
      "Requirement already satisfied: flatbuffers==25.2.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 17)) (25.2.10)\n",
      "Requirement already satisfied: fonttools==4.55.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 18)) (4.55.3)\n",
      "Requirement already satisfied: frozenlist==1.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 19)) (1.5.0)\n",
      "Requirement already satisfied: fsspec==2024.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 20)) (2024.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 21)) (0.4.0)\n",
      "Requirement already satisfied: google-auth==2.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 22)) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 23)) (0.4.6)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 24)) (0.2.0)\n",
      "Requirement already satisfied: grpcio==1.70.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 25)) (1.70.0)\n",
      "Requirement already satisfied: h5py==3.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 26)) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub==0.29.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 27)) (0.29.1)\n",
      "Requirement already satisfied: idna==3.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 28)) (3.10)\n",
      "Requirement already satisfied: importlib_metadata==8.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 29)) (8.6.1)\n",
      "Requirement already satisfied: importlib_resources==6.4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 30)) (6.4.5)\n",
      "Requirement already satisfied: joblib==1.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 31)) (1.4.2)\n",
      "Requirement already satisfied: keras==2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 32)) (2.10.0)\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 33)) (1.1.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 34)) (1.4.7)\n",
      "Requirement already satisfied: libclang==18.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 35)) (18.1.1)\n",
      "Requirement already satisfied: Markdown==3.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 36)) (3.7)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 37)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.8.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 38)) (3.8.4)\n",
      "Requirement already satisfied: multidict==6.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 39)) (6.1.0)\n",
      "Requirement already satisfied: multiprocess==0.70.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 40)) (0.70.16)\n",
      "Requirement already satisfied: numpy==1.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 41)) (1.23.0)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 42)) (3.2.2)\n",
      "Requirement already satisfied: opt_einsum==3.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 43)) (3.4.0)\n",
      "Requirement already satisfied: packaging==24.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 44)) (24.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 45)) (2.2.2)\n",
      "Requirement already satisfied: pillow==11.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 46)) (11.0.0)\n",
      "Requirement already satisfied: propcache==0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 47)) (0.3.0)\n",
      "Requirement already satisfied: protobuf==3.19.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 48)) (3.19.6)\n",
      "Requirement already satisfied: pyarrow==19.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 49)) (19.0.1)\n",
      "Requirement already satisfied: pyasn1==0.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 50)) (0.6.1)\n",
      "Requirement already satisfied: pyasn1_modules==0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 51)) (0.4.1)\n",
      "Requirement already satisfied: pyparsing==3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 52)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 53)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-version==0.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 54)) (0.0.2)\n",
      "Requirement already satisfied: pytz==2024.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 55)) (2024.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 56)) (6.0.2)\n",
      "Requirement already satisfied: regex==2024.11.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 57)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 58)) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib==2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 59)) (2.0.0)\n",
      "Requirement already satisfied: rsa==4.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 60)) (4.9)\n",
      "Requirement already satisfied: safetensors==0.5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 61)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 62)) (1.5.0)\n",
      "Requirement already satisfied: scipy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 63)) (1.13.1)\n",
      "Requirement already satisfied: six==1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 64)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard==2.10.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 65)) (2.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 66)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 67)) (1.8.1)\n",
      "Collecting tensorflow==2.10.0rc3\n",
      "  Using cached tensorflow-2.10.0rc3-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "Requirement already satisfied: tensorflow-cpu==2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 69)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-gpu==2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 70)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator==2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 71)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.31.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 72)) (0.31.0)\n",
      "Requirement already satisfied: tensorflow_intel==2.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 73)) (2.10.0)\n",
      "Requirement already satisfied: termcolor==2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 74)) (2.5.0)\n",
      "Requirement already satisfied: tokenizers==0.15.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 75)) (0.15.2)\n",
      "Requirement already satisfied: transformers==4.37.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 76)) (4.37.2)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 77)) (4.12.2)\n",
      "Requirement already satisfied: tzdata==2024.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 78)) (2024.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 79)) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug==3.1.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 80)) (3.1.3)\n",
      "Requirement already satisfied: wrapt==1.17.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 81)) (1.17.2)\n",
      "Requirement already satisfied: xxhash==3.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 82)) (3.5.0)\n",
      "Requirement already satisfied: yarl==1.18.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 83)) (1.18.3)\n",
      "Requirement already satisfied: zipp==3.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 84)) (3.21.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse==1.6.3->-r requirements.txt (line 5)) (0.43.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2->-r requirements.txt (line 14)) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn==1.5.0->-r requirements.txt (line 62)) (3.5.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard==2.10.1->-r requirements.txt (line 65)) (49.2.1)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.10.0rc3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (1.23.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (3.17.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.12.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.37.2) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.37.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.37.2) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.37.2) (3.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==3.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (3.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (1.23.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (3.11.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (3.17.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (0.29.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (0.3.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (24.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.3.2) (19.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets==3.3.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets==3.3.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets==3.3.2) (2024.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (1.18.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (2.4.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (0.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.3.2) (25.1.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.66.3->datasets==3.3.2) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.3.2) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.3.2) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.3.2) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.3.2) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.3.2) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creation of model for image classification of animals i need labeled dataset of animals' images that consist of, at least, 10 classes. I found one of them on kaggle, credit: <url> https://www.kaggle.com/datasets/utkarshsaxenadn/animal-image-classification-dataset</url>. During the training, I used only 100 images for each animal. Below is a class of model for image classification. As we can see, the model can be saved in .h5 format, but github does not allow to save files more than 25 mb, so we need to train the model each time.\n",
    "\n",
    "Architecture of the model consist of: \n",
    "* input of (64,64,1) tensor, \n",
    "\n",
    "* Convolution layer with 32 filters of (3,3) size with relu activation, \n",
    "\n",
    "* MaxPooling layer, \n",
    "\n",
    "* Convoluition layer with 64 filters of (3,3) with relu activation, \n",
    "\n",
    "* MaxPooling layer, \n",
    "\n",
    "* Convolution layer with 128 filters of (3,3) size with relu activation, \n",
    "\n",
    "* MaxPooling layer, \n",
    "\n",
    "* Convoluition layer with 256 filters of (3,3) with relu activation, \n",
    "\n",
    "* MaxPooling layer, \n",
    "\n",
    "* Flatten layer and then Dense layers with 512, 256, 128 neurons, respectively, with relu as activation function, \n",
    "\n",
    "* The final layer is a dense of 15 neurons with softmax as activation function. \n",
    "\n",
    "As a loss function for training of this model was used the \"sparse_categorical_crossentropy\" and Adam as an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input, Conv2D,MaxPooling2D,Conv3D,MaxPooling3D\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "class CNNClassifier:\n",
    "    \"\"\"\n",
    "    Convolution Neural Network model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path=None):\n",
    "        if path is None:\n",
    "            input = Input(shape=(64,64,1), dtype=tf.float32)\n",
    "\n",
    "            conv1 = Conv2D(32, (3, 3), activation='relu', name='conv_layer_1')(input)\n",
    "            pool1 = MaxPooling2D((2, 2), name='maxpool_layer_1')(conv1)\n",
    "            conv2 = Conv2D(64, (3, 3), activation='relu', name='conv_layer_2')(pool1)\n",
    "            pool2 = MaxPooling2D((2, 2), name='maxpool_layer_2')(conv2)\n",
    "            conv3 = Conv2D(128, (3, 3), activation='relu', name='conv_layer_3')(pool2)\n",
    "            pool3 = MaxPooling2D((2, 2), name='maxpool_layer_3')(conv3)\n",
    "            conv4 = Conv2D(256, (3, 3), activation='relu', name='conv_layer_4')(pool3)\n",
    "            pool4 = MaxPooling2D((2, 2), name='maxpool_layer_4')(conv4)\n",
    "\n",
    "            flatten = Flatten()(pool2)\n",
    "            dense1 = Dense(512, activation='relu', name='dense_layer_1')(flatten)\n",
    "            dense2 = Dense(256, activation='relu', name='dense_layer_2')(dense1)\n",
    "            dense3 = Dense(128, activation='relu', name='dense_layer_3')(dense2)\n",
    "            output_layer = Dense(15, activation='softmax', name='output_layer')(dense3)\n",
    "            self.model = Model(inputs=input, outputs=output_layer)\n",
    "            self.model.summary()\n",
    "        else:\n",
    "            self.model=load_model('image_classification_model.h5')\n",
    "    def train(self,x_train,y_train):\n",
    "        x_train=x_train.reshape((x_train.shape[0],64, 64,1))\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(x_train, y_train,epochs=10, batch_size=32)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        x_test = x_test.reshape((x_test.shape[0], 64, 64,1))\n",
    "        y_pred = self.model.predict(x_test)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def save(self,path):\n",
    "        self.model.save(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aftter creation of model's class, we can train it. We read directories of \"train_data\" and \"test_data\", where images separated into animals. As I said, for each animal i use only 100 images. The output of the model is an animal index (model is similar to the Convolution Neural Network in the Task 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing is finished.\n",
      "LabelEncoding is finished.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
      "                                                                 \n",
      " conv_layer_1 (Conv2D)       (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " maxpool_layer_1 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_layer_2 (Conv2D)       (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " maxpool_layer_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_layer_1 (Dense)       (None, 512)               6423040   \n",
      "                                                                 \n",
      " dense_layer_2 (Dense)       (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_layer_3 (Dense)       (None, 128)               32896     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 15)                1935      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,608,015\n",
      "Trainable params: 6,608,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Start training.\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 6s 63ms/step - loss: 2.5846 - accuracy: 0.1390\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 6s 63ms/step - loss: 2.1658 - accuracy: 0.2813\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 6s 63ms/step - loss: 1.8833 - accuracy: 0.3857\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 6s 66ms/step - loss: 1.5234 - accuracy: 0.5144\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 6s 66ms/step - loss: 1.1089 - accuracy: 0.6461\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 6s 64ms/step - loss: 0.7338 - accuracy: 0.7645\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 6s 65ms/step - loss: 0.4769 - accuracy: 0.8537\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 6s 67ms/step - loss: 0.2414 - accuracy: 0.9323\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 6s 67ms/step - loss: 0.1402 - accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 6s 67ms/step - loss: 0.0726 - accuracy: 0.9831\n",
      "48/48 [==============================] - 1s 14ms/step\n",
      "Accuracy score for test data: 0.9953795379537954\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from image_classification_model import CNNClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "train_path='train_data/'\n",
    "test_path='test_data/'\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "def load_img(img):\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.image.decode_jpeg(img,channels=1)\n",
    "    img=tf.cast(img,tf.float32)/255\n",
    "    img=tf.image.resize(img,(64,64))\n",
    "    return img\n",
    "\n",
    "def import_folder(path,max_count=100):\n",
    "    x_data,y_data=[],[]\n",
    "    for animal_folder in os.listdir(train_path):\n",
    "        count=0\n",
    "        for image_path in os.listdir(os.path.join(train_path,animal_folder)):\n",
    "            if count>max_count:\n",
    "                break\n",
    "            x_data.append(load_img(os.path.join(train_path,animal_folder,image_path)))\n",
    "            y_data.append(str.lower(animal_folder))\n",
    "            count+=1\n",
    "\n",
    "    return np.array(x_data),np.array(y_data)\n",
    "\n",
    "\n",
    "\n",
    "x_train,y_train=import_folder(train_path,200)\n",
    "x_test,y_test=import_folder(test_path)\n",
    "print('Importing is finished.')\n",
    "\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y_train=labelencoder.fit_transform(y_train)\n",
    "y_test=labelencoder.transform(y_test)\n",
    "print('LabelEncoding is finished.')\n",
    "\n",
    "\n",
    "model=CNNClassifier()\n",
    "\n",
    "print('\\nStart training.')\n",
    "model.train(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "model.save('image_classification_model.h5')\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score for test data: {acc_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created an image classification model, we can move on to the next part is the Name Entity Recognition model.\n",
    "\n",
    "This model was created with finetuning of the BERT model from HuggingFace. Dataset for finetuning was collected with generative services, which generate sentences that include animals' name we want to recognize. \n",
    "\n",
    "For creation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 207/207 [00:00<00:00, 3779.56 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 2346.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForTokenClassification,BertTokenizerFast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "##DATASET\n",
    "######################################################################################################################################################################################################################################\n",
    "train_texts = [\n",
    "    \"I saw a beetle in the garden today.\",\n",
    "    \"The butterfly gracefully fluttered by.\",\n",
    "    \"My cat loves to sleep on the couch.\",\n",
    "    \"We have a cow in our farm.\",\n",
    "    \"The dog barked loudly at the mailman.\",\n",
    "    \"An elephant is a majestic creature.\",\n",
    "    \"I saw a gorilla at the zoo.\",\n",
    "    \"The hippo was swimming in the river.\",\n",
    "    \"A lizard was sunbathing on the rock.\",\n",
    "    \"The monkey swung from tree to tree.\",\n",
    "    \"A mouse scurried across the floor.\",\n",
    "    \"The panda was eating bamboo.\",\n",
    "    \"There was a spider in the corner of the room.\",\n",
    "    \"The tiger roared in the jungle.\",\n",
    "    \"A zebra galloped across the savannah.\",\n",
    "    \"The beetle crawled under the leaves.\",\n",
    "    \"A butterfly danced above the meadow.\",\n",
    "    \"The cat napped in the sunbeam.\",\n",
    "    \"The cow stood by the fence.\",\n",
    "    \"The dog chased after a rabbit.\",\n",
    "    \"An elephant shook the ground with its footsteps.\",\n",
    "    \"The gorilla groomed its fur.\",\n",
    "    \"The hippo wallowed in the mud.\",\n",
    "    \"A lizard scampered up the tree.\",\n",
    "    \"The monkey clambered up the branches.\",\n",
    "    \"A mouse darted into a crevice.\",\n",
    "    \"The panda lounged under the tree.\",\n",
    "    \"A spider crawled along the wall.\",\n",
    "    \"The tiger crouched in the tall grass.\",\n",
    "    \"The zebra sprinted across the savannah.\",\n",
    "    \"The beetle hid under the bark.\",\n",
    "    \"The butterfly rested on a petal.\",\n",
    "    \"The cat licked its paws.\",\n",
    "    \"The cow drank from the pond.\",\n",
    "    \"The dog howled at the moon.\",\n",
    "    \"An elephant walked through the clearing.\",\n",
    "    \"The gorilla sat under the shade.\",\n",
    "    \"The hippo emerged from the water.\",\n",
    "    \"A lizard lay motionless on the stone.\",\n",
    "    \"The monkey played with the leaves.\",\n",
    "    \"A mouse squeaked and ran away.\",\n",
    "    \"The panda lay on its back.\",\n",
    "    \"A spider spun an intricate web.\",\n",
    "    \"The tiger's eyes gleamed in the night.\",\n",
    "    \"The zebra's stripes shimmered in the sun.\",\n",
    "    \"The beetle scurried across the field.\",\n",
    "    \"The butterfly floated above the grass.\",\n",
    "    \"The cat curled up on the bed.\",\n",
    "    \"The cow wandered through the meadow.\",\n",
    "    \"The dog sniffed around the yard.\",\n",
    "    \"An elephant picked up a branch with its trunk.\",\n",
    "    \"The gorilla looked out from its enclosure.\",\n",
    "    \"The hippo splashed in the shallow waters.\",\n",
    "    \"A lizard ran up the garden wall.\",\n",
    "    \"The monkey grabbed a handful of fruit.\",\n",
    "    \"A mouse chewed on a small piece of food.\",\n",
    "    \"The panda lounged in the bamboo grove.\",\n",
    "    \"A spider climbed up the tree trunk.\",\n",
    "    \"The tiger's fur rippled as it moved.\",\n",
    "    \"The zebra grazed with other animals.\",\n",
    "    \"The beetle explored the undergrowth.\",\n",
    "    \"The butterfly hovered near the blossoms.\",\n",
    "    \"The cat chased a feather toy.\",\n",
    "    \"The cow lowed softly in the barn.\",\n",
    "    \"The dog panted after a long run.\",\n",
    "    \"An elephant trumpeted at the waterhole.\",\n",
    "    \"The gorilla sat and observed.\",\n",
    "    \"The hippo submerged to keep cool.\",\n",
    "    \"A lizard darted into a crack.\",\n",
    "    \"The monkey picked fruit from the tree.\",\n",
    "    \"A mouse hid from the light.\",\n",
    "    \"The panda sat munching on bamboo.\",\n",
    "    \"A spider waited in its web for prey.\",\n",
    "    \"The tiger crouched, ready to pounce.\",\n",
    "    \"The zebra galloped across the plain.\",\n",
    "    \"The beetle burrowed into the ground.\",\n",
    "    \"The butterfly fluttered past the roses.\",\n",
    "    \"The cat purred contentedly.\",\n",
    "    \"The cow roamed the pasture.\",\n",
    "    \"The dog rolled in the grass.\",\n",
    "    \"The hippo opened its large mouth.\",\n",
    "    \"A lizard scurried up the tree.\",\n",
    "    \"The monkey ate a banana.\",\n",
    "    \"A mouse hid under the bed.\",\n",
    "    \"The panda climbed the tree.\",\n",
    "    \"There was a spider web in the corner.\",\n",
    "    \"The tiger prowled through the jungle.\",\n",
    "    \"The zebra grazed on the grass.\",\n",
    "    \"The beetle scurried under the fallen log.\",\n",
    "    \"A butterfly rested on the daisy.\",\n",
    "    \"The cat chased its own tail in circles.\",\n",
    "    \"The cow lay down in the shade.\",\n",
    "    \"The dog fetched the ball enthusiastically.\",\n",
    "    \"An elephant splashed water with its trunk.\",\n",
    "    \"The gorilla ate bananas in the enclosure.\",\n",
    "    \"The hippo yawned widely, showing its large teeth.\",\n",
    "    \"A lizard climbed up the wall effortlessly.\",\n",
    "    \"The monkey stole a piece of fruit.\",\n",
    "    \"A mouse peeked out of its hole.\",\n",
    "    \"The panda slept soundly on a tree branch.\",\n",
    "    \"A spider hung from its silk thread.\",\n",
    "    \"The tiger's stripes blended into the foliage.\",\n",
    "    \"The zebra's stripes created a beautiful pattern.\",\n",
    "    \"The beetle climbed the stem of a plant.\",\n",
    "    \"The butterfly rested on a sunflower.\",\n",
    "    \"The cat hunted a toy mouse.\",\n",
    "    \"The cow drank from the trough.\",\n",
    "    \"The dog barked at passing cars.\",\n",
    "    \"An elephant bathed in the river.\",\n",
    "    \"The gorilla foraged for fruit.\",\n",
    "    \"The hippo dozed by the water's edge.\",\n",
    "    \"A lizard hid among the rocks.\",\n",
    "    \"The monkey swung from vine to vine.\",\n",
    "    \"A mouse nibbled on some cheese.\",\n",
    "    \"The panda tumbled playfully.\",\n",
    "    \"A spider crawled on the ceiling.\",\n",
    "    \"The tiger watched its surroundings closely.\",\n",
    "    \"The zebra stood with its herd.\"\n",
    "   \"The Tiger prowled silently through the tall grass.\",\n",
    "    \"The Zebra's stripes are unique and beautiful.\" \"I saw a Beetle in the garden today.\",\n",
    "    \"The Butterfly gracefully fluttered by.\", \"My Cat loves to sleep on the couch.\",\n",
    "    \"We have a Cow in our farm.\", \"The Dog barked loudly at the mailman.\",\n",
    "    \"An Elephant is a majestic creature.\", \"I saw a Gorilla at the zoo.\",\n",
    "    \"The Hippo was swimming in the river.\", \"A Lizard was sunbathing on the rock.\",\n",
    "    \"The Monkey swung from tree to tree.\", \"A Mouse scurried across the floor.\",\n",
    "    \"The Panda was eating bamboo.\", \"There was a Spider in the corner of the room.\",\n",
    "    \"The Tiger roared in the jungle.\", \"A Zebra galloped across the savannah.\",\n",
    "    \"The Beetle crawled across the leaf.\", \"The Butterfly danced in the breeze.\",\n",
    "    \"Our Cat purred softly on my lap.\", \"The Cow mooed gently in the field.\",\n",
    "    \"The Dog wagged its tail excitedly.\", \"The Elephant trumpeted loudly in the savannah.\",\n",
    "    \"The Gorilla swung from branch to branch.\", \"The Hippo lounged in the muddy water.\",\n",
    "    \"A Lizard darted across the patio.\", \"The Monkey chattered noisily in the treetops.\",\n",
    "    \"A Mouse nibbled on a piece of cheese.\", \"The Panda rolled playfully on the grass.\",\n",
    "    \"A Spider spun a web in the garden.\", \"The Tiger stalked its prey silently.\",\n",
    "    \"The Beetle scurried under the fallen log.\", \"A Butterfly rested on the daisy.\",\n",
    "    \"The Cat chased its own tail in circles.\", \"The Cow lay down in the shade.\",\n",
    "    \"The Dog fetched the ball enthusiastically.\", \"An Elephant splashed water with its trunk.\",\n",
    "    \"The Gorilla ate bananas in the enclosure.\", \"The Hippo yawned widely, showing its large teeth.\",\n",
    "    \"A Lizard climbed up the wall effortlessly.\", \"The Monkey stole a piece of fruit.\",\n",
    "    \"A Mouse peeked out of its hole.\", \"The Panda slept soundly on a tree branch.\",\n",
    "    \"A Spider hung from its silk thread.\", \"The Tiger's stripes blended into the foliage.\",\n",
    "    \"The Zebra's stripes were mesmerizing.\", \"The Beetle scuttled across the floor.\",\n",
    "    \"The Butterfly gently landed on the flower.\", \"The Cat meowed and rubbed against my leg.\",\n",
    "    \"The Cow chewed cud peacefully.\", \"The Dog ran after the frisbee.\", \"The Elephant raised its trunk to trumpet.\",\n",
    "    \"The Gorilla played with its young.\", \"The Hippo submerged itself in the water.\",\n",
    "    \"A Lizard basked in the sunlight.\", \"The Monkey swung from branch to branch.\",\n",
    "    \"A Mouse sniffed around for food.\", \"The Panda lazily ate bamboo.\", \"A Spider crawled along its web.\",\n",
    "    \"The Tiger prowled through the jungle.\", \"The Zebra trotted across the plain.\", \"The Beetle hid under a rock.\",\n",
    "    \"The Butterfly fluttered in the meadow.\", \"The Cat purred while napping.\", \"The Cow grazed in the pasture.\",\n",
    "    \"The Dog barked at the stranger.\", \"The Elephant trumpeted loudly.\", \"The Gorilla climbed a tree.\",\n",
    "    \"The Hippo lounged by the riverbank.\", \"A Lizard darted across the sand.\", \"The Monkey chattered excitedly.\",\n",
    "    \"A Mouse found some crumbs.\", \"The Panda rolled down the hill.\", \"A Spider spun its web in the corner.\",\n",
    "    \"The Tiger leapt gracefully.\", \"The Zebra stood near the watering hole.\", \"The Beetle explored the garden.\",\n",
    "    \"The Butterfly landed on a leaf.\", \"The Cat stretched and yawned.\", \"The Cow lay in the shade of a tree.\",\n",
    "    \"The Dog wagged its tail happily.\",\n",
    "    \"An Elephant waded through the water.\", \"The Gorilla beat its chest.\", \"The Hippo opened its massive mouth.\",\n",
    "    \"A Lizard sunned itself on a rock.\",\n",
    "    \"The Monkey swung from a vine.\", \"A Mouse hid in a small hole.\", \"The Panda ate a large bamboo shoot.\",\n",
    "    \"A Spider dangled from its thread.\",\n",
    "    \"The Tiger prowled silently.\",\n",
    "    \"The Zebra grazed with its herd.\"\n",
    "\n",
    "]\n",
    "\n",
    "test_texts=[\n",
    "    \"The Tiger stalked its prey.\",\n",
    "    \"A Zebra was drinking from the river.\",\n",
    "    \"I don't believe there is a Unicorn in the room.\",\n",
    "    \"The Rabbit dug a hole in the ground.\",\n",
    "    \"We couldn't find a Bear anywhere.\",\n",
    "    \"The Fish glided through the water.\",\n",
    "    \"The Parrot squawked loudly.\",\n",
    "    \"The Dolphin swam near the boat.\",\n",
    "    \"There is absolutely no Alligator here.\",\n",
    "    \"The Kangaroo had a joey in its pouch.\",\n",
    "    \"I didn't notice any Deer around.\",\n",
    "    \"The Sheep were gathered near the fence.\",\n",
    "    \"The Owl sat on the branch.\",\n",
    "    \"The Peacock strutted around the yard.\"]\n",
    "animals = [\n",
    "    \"Eagle\", \"Robin\", \"Fox\", \"Hawk\", \"Elk\", \"Seal\", \"Snake\", \"Chickens\", \"Wolf\", \"Bison\", \"Pigeon\",\n",
    "    \"Otter\", \"Whale\", \"Canary\", \"Panther\", \"Antelope\", \"Hedgehog\", \"Frog\", \"Bee\", \"Lynx\", \"Buffalo\",\n",
    "    \"Coyote\", \"Mole\", \"Mountain Goat\", \"Sparrows\", \"Raccoon\", \"Rooster\", \"Pig\", \"Woodpecker\", \"Falcon\",\n",
    "    \"Lemur\", \"Crab\", \"Skylark\", \"Iguana\", \"Duck\", \"Snail\", \"Gecko\", \"Chipmunk\", \"Tarantula\", \"Osprey\",\n",
    "    \"Kingfisher\", \"Kestrel\", \"Ladybug\", \"Heron\", \"Python\", \"Salamander\", \"Warbler\", \"Dragonfly\",\n",
    "    \"Caterpillar\", \"Jaguar\", \"Tortoise\", \"Flamingo\", \"Gibbon\", \"Capybara\", \"Gazelle\", \"Dormouse\",\n",
    "    \"Finch\", \"Crocodile\", \"Snow Leopard\", \"Howler Monkey\", \"Water Buffalo\", \"Leopard\", \"Horse\",\n",
    "    \"Gull\", \"Chameleon\"\n",
    "]\n",
    "\n",
    "########################################################################################################################################################################################################################################################################################################################\n",
    "def encode_message(message, animals):\n",
    "    words = message.split()\n",
    "    encoded_words = [1 if any(animal in word for animal in animals) else 0 for word in words]\n",
    "    return encoded_words\n",
    "\n",
    "train_labels = [encode_message(text, animals) for text in train_texts] #Create labels for each word\n",
    "test_labels = [encode_message(text, animals) for text in test_texts]\n",
    "\n",
    "def encode_texts(label):\n",
    "    labels_encoding={0:\"O\", 1: \"B-ANIMAL\"}\n",
    "    return [labels_encoding[label[i]] for i in range(len(label))]\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_texts=np.array(train_texts)\n",
    "test_texts=np.array(test_texts)\n",
    "\n",
    "train_ner_tags=[encode_texts(label) for label in train_labels] #set labels to words \n",
    "test_ner_tags=[encode_texts(label) for label in test_labels]\n",
    "\n",
    "train_tokens=[tokenizer.tokenize(text) for i,text in enumerate(train_texts)] #tokenize sentences in dataset\n",
    "test_tokens=[tokenizer.tokenize(text) for i,text in enumerate(test_texts)]\n",
    "\n",
    "train_dataset=Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels, \"tokens\": train_tokens,'ner_tags':train_ner_tags}) #create a dataset from processed labels, tokens and sentences.\n",
    "test_dataset=Dataset.from_dict({\"text\": test_texts, \"labels\": test_labels, \"tokens\": test_tokens,'ner_tags':test_ner_tags})\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset,\n",
    "                             \"test\": test_dataset}) #combine train and test dataset into under one variable.\n",
    "\n",
    "                                                                                                                                                                \n",
    "def tokenize_and_align_labels(examples,label_all_tokens=True):\n",
    "#This function is dedicated to assign a label to tokens that we received after possible word decomposition into tokens: for example, word \"beetle\" can be divided into \"bee\" and \"**tle\" and word \"bee\" will marked as \"animal\" - 1,\n",
    "#other tokens can be 0. These arrays of marks is replacing Y_array for us and finetuned moddel should classify inputed  sentences as in the marked array. \n",
    "\n",
    "#For successful finetuning we still need to make equal size of those Y_arrays of marks. So, we expand size to the maximum and fill new elements with  0, for Torch - \"-100\".\n",
    "# \n",
    "#  \n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    max_length=24\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        current_word = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx != current_word:\n",
    "                current_word=word_idx\n",
    "                label_temp=0 if (word_idx is None) or (word_idx >= len(label)) else label[word_idx]\n",
    "                label_ids.append(label_temp)\n",
    "\n",
    "            elif word_idx is None:\n",
    "                label_ids.append(0)\n",
    "            else:\n",
    "                label_ids.append(0)\n",
    "\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        max_length = max(max_length, len(label_ids))\n",
    "\n",
    "    tokenized_inputs['input_ids'] = pad_sequences(tokenized_inputs['input_ids'], maxlen=max_length, padding=\"post\", value=0)\n",
    "    labels = pad_sequences(labels, maxlen=max_length, padding=\"post\", value=0) #make the same size for each label in dataset \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "#make the same size for each array in dataset. We need to create extra values that will be filled with \"0\".\n",
    "def pad_to_max_length(sequences, max_length, padding_value=0):\n",
    "    return [seq + [padding_value] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "\n",
    "max_length_train = max(len(seq) for seq in tokenized_dataset[\"train\"][\"input_ids\"])\n",
    "max_length_test = max(len(seq) for seq in tokenized_dataset[\"test\"][\"input_ids\"])\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "\n",
    "train_inputs = tokenized_dataset[\"train\"][\"input_ids\"]\n",
    "train_attention_mask = pad_to_max_length(tokenized_dataset[\"train\"][\"attention_mask\"], max_length, padding_value=0)\n",
    "train_labels = tokenized_dataset[\"train\"][\"labels\"]\n",
    "\n",
    "test_inputs = tokenized_dataset[\"test\"][\"input_ids\"]\n",
    "test_attention_mask = pad_to_max_length(tokenized_dataset[\"test\"][\"attention_mask\"], max_length, padding_value=0)\n",
    "test_labels = tokenized_dataset[\"test\"][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Name Entity Recognition training.</b>\n",
    "\n",
    "After preparing dataset for training, we can fit it into BERT model and save it. Model will be saved in separate directory. Model was trained with batches of 2 sentences using Adam optimizer and sparse_category_crossentropy as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 71s 537ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9968\n",
      "7/7 [==============================] - 3s 70ms/step\n",
      "7/7 [==============================] - 3s 68ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./trained_ner_model\\\\tokenizer_config.json',\n",
       " './trained_ner_model\\\\special_tokens_map.json',\n",
       " './trained_ner_model\\\\vocab.txt',\n",
       " './trained_ner_model\\\\added_tokens.json',\n",
       " './trained_ner_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": train_inputs, \"attention_mask\": train_attention_mask},\n",
    "    train_labels\n",
    ")).batch(2)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": test_inputs, \"attention_mask\": test_attention_mask},\n",
    "    test_labels\n",
    ")).batch(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TFBertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.fit(train_data, epochs=1)\n",
    "predictions = model.predict(test_data)\n",
    "u=model.evaluate(test_data)\n",
    "logits = predictions.logits\n",
    "\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "rf=predicted_labels-test_labels\n",
    "\n",
    "model.save_pretrained(\"./trained_ner_model\")\n",
    "tokenizer.save_pretrained(\"./trained_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pipeline </b>\n",
    "\n",
    "Below is shown a  Pipeline class, we can see that we load trained models during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForTokenClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "class NERmodel():\n",
    "    def __init__(self,path_to_model):\n",
    "        self.model=TFBertForTokenClassification.from_pretrained(path_to_model)\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(path_to_model)\n",
    "        # optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #\n",
    "        # self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        print('Model is loaded.')\n",
    "    def train(self,train_dataset):\n",
    "\n",
    "        pass\n",
    "    def predict(self,test_dataset):\n",
    "\n",
    "        inputs = self.tokenizer(test_dataset, return_tensors=\"tf\")\n",
    "        output=self.model(**inputs)\n",
    "        logits=np.argmax(output.logits,axis=-1)[0]\n",
    "        max_index=np.argmax(logits)\n",
    "        # token_id=\n",
    "        tokens=self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        return tokens[max_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./trained_ner_model were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at ./trained_ner_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./trained_ner_model were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at ./trained_ner_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Image predict: dog.\n",
      "\n",
      "Token predict: [cls].\n",
      "\n",
      "Result of pipeline: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from pipeline import Pipeline_for_task_2\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "from ner_model_class import NERmodel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "path_to_image1='./Dog-Test (9).jpg'\n",
    "path_to_image2='./Elephant-Test (380).jpeg'\n",
    "path_to_image3='./Cow-Test (15).jpeg'\n",
    "\n",
    "string= \"The Unicorn pranced under the rainbow.\"\n",
    "\n",
    "text1=\"My dog loves to sleep on the couch.\"\n",
    "text2=\"An elephant walked through the clearing.\"\n",
    "text3=\"The cow drank from the pond.\"\n",
    "\n",
    "\n",
    "\n",
    "classification_model_path='image_classification_model.h5'\n",
    "entity_recognition_model_path='./trained_ner_model'\n",
    "\n",
    "\n",
    "pipeline=Pipeline_for_task_2(classification_model_path,entity_recognition_model_path)\n",
    "\n",
    "\n",
    "image=np.asarray(Image.open(fr\"{path_to_image1}\").convert('L').resize((64,64)))\n",
    "\n",
    "ner_model=NERmodel(entity_recognition_model_path)\n",
    "\n",
    "image=image.reshape((-1, 64, 64,1))\n",
    "\n",
    "result=pipeline.evaluate(image,text1)\n",
    "\n",
    "print(f'Result of pipeline: {result}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
